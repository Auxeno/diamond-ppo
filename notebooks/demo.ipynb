{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ’Ž Diamond PPO Demo\n",
    "\n",
    "This notebook demonstrates the core features of Diamond PPO, a lightweight PyTorch implementation of Proximal Policy Optimization.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/auxeno/diamond-ppo/blob/main/notebooks/demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, let's install Diamond PPO and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Diamond PPO from GitHub\n",
    "!pip install -q git+https://github.com/auxeno/diamond-ppo\n",
    "\n",
    "# Install additional dependencies for visualization\n",
    "!apt-get install -qq xvfb\n",
    "!pip install -q pyvirtualdisplay pygame moviepy imageio\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import io\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Gymnasium version: {gym.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Usage - Discrete Actions\n",
    "\n",
    "Let's start with a simple example using CartPole, a classic control task with discrete actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diamond import PPO, PPOConfig\n",
    "\n",
    "# Create a simple PPO agent with custom configuration\n",
    "config = PPOConfig(\n",
    "    total_steps=50_000,  # Total training steps\n",
    "    rollout_steps=128,   # Steps per rollout\n",
    "    num_envs=4,          # Parallel environments\n",
    "    lr=3e-4,             # Learning rate\n",
    "    gamma=0.99,          # Discount factor\n",
    "    gae_lambda=0.95,     # GAE lambda\n",
    "    ppo_clip=0.2,        # PPO clipping parameter\n",
    "    verbose=True         # Print training progress\n",
    ")\n",
    "\n",
    "# Create and train the agent\n",
    "agent = PPO(\n",
    "    env_fn=lambda: gym.make(\"CartPole-v1\"),\n",
    "    cfg=config\n",
    ")\n",
    "\n",
    "print(\"Training PPO on CartPole...\")\n",
    "agent.train()\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Continuous Control\n",
    "\n",
    "For continuous action spaces, use `ContinuousPPO`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diamond import ContinuousPPO, ContinuousPPOConfig\n",
    "\n",
    "# Configure continuous PPO\n",
    "config = ContinuousPPOConfig(\n",
    "    total_steps=100_000,\n",
    "    rollout_steps=256,\n",
    "    num_envs=4,\n",
    "    lr=3e-4,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Train on a continuous control task\n",
    "agent = ContinuousPPO(\n",
    "    env_fn=lambda: gym.make(\"Pendulum-v1\"),\n",
    "    cfg=config\n",
    ")\n",
    "\n",
    "print(\"Training Continuous PPO on Pendulum...\")\n",
    "agent.train()\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Neural Networks\n",
    "\n",
    "Diamond PPO supports custom network architectures. Here's an example with a larger network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        super().__init__()\n",
    "        # Shared feature extractor\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Separate heads for actor and critic\n",
    "        self.actor_head = nn.Linear(128, action_dim)\n",
    "        self.critic_head = nn.Linear(128, 1)\n",
    "    \n",
    "    def actor(self, x):\n",
    "        \"\"\"Returns action logits\"\"\"\n",
    "        features = self.base(x)\n",
    "        return self.actor_head(features)\n",
    "    \n",
    "    def critic(self, x):\n",
    "        \"\"\"Returns value estimates\"\"\"\n",
    "        features = self.base(x)\n",
    "        return self.critic_head(features).squeeze(-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns both actor and critic outputs\"\"\"\n",
    "        features = self.base(x)\n",
    "        return self.actor_head(features), self.critic_head(features).squeeze(-1)\n",
    "\n",
    "# Use the custom network\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "env.close()\n",
    "\n",
    "custom_net = CustomNetwork(obs_dim, action_dim)\n",
    "\n",
    "config = PPOConfig(\n",
    "    total_steps=30_000,\n",
    "    rollout_steps=128,\n",
    "    num_envs=4,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent = PPO(\n",
    "    env_fn=lambda: gym.make(\"CartPole-v1\"),\n",
    "    cfg=config,\n",
    "    custom_network=custom_net\n",
    ")\n",
    "\n",
    "print(\"Training with custom network architecture...\")\n",
    "agent.train()\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Utilities\n",
    "\n",
    "Diamond PPO includes helpful utilities for monitoring training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diamond.utils import Logger, Timer\n",
    "\n",
    "# Example of using the Logger\n",
    "logger = Logger()\n",
    "\n",
    "# Simulate some training metrics\n",
    "np.random.seed(42)\n",
    "for step in range(100):\n",
    "    # Simulated metrics\n",
    "    reward = 100 + step * 2 + np.random.randn() * 10\n",
    "    loss = 1.0 / (1 + step * 0.1) + np.random.randn() * 0.01\n",
    "    \n",
    "    logger.log(\"episode_reward\", step, reward)\n",
    "    logger.log(\"policy_loss\", step, loss)\n",
    "\n",
    "# Plot the logged metrics\n",
    "print(\"Episode Rewards:\")\n",
    "logger.plot(\"episode_reward\")\n",
    "print(\"\\nPolicy Loss:\")\n",
    "logger.plot(\"policy_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the Timer for profiling\n",
    "from diamond.utils import Timer\n",
    "import time\n",
    "\n",
    "timer = Timer()\n",
    "\n",
    "# Simulate different parts of a training loop\n",
    "for i in range(5):\n",
    "    with timer.time(\"environment_step\"):\n",
    "        time.sleep(0.01)  # Simulate env.step()\n",
    "    \n",
    "    with timer.time(\"network_forward\"):\n",
    "        time.sleep(0.005)  # Simulate network forward pass\n",
    "    \n",
    "    with timer.time(\"optimization\"):\n",
    "        time.sleep(0.008)  # Simulate optimization step\n",
    "\n",
    "# Display timing statistics\n",
    "timer.plot_timings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation and Visualization\n",
    "\n",
    "Let's evaluate a trained agent and visualize its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env_name, num_episodes=5, render=False):\n",
    "    \"\"\"Evaluate a trained agent.\"\"\"\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\" if render else None)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    frames = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Get action from trained network\n",
    "            with torch.no_grad():\n",
    "                obs_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "                if hasattr(agent, 'network'):\n",
    "                    logits, _ = agent.network(obs_tensor)\n",
    "                    action = torch.argmax(logits, dim=-1).item()\n",
    "                else:\n",
    "                    # For demonstration, use random actions\n",
    "                    action = env.action_space.sample()\n",
    "            \n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if render and episode == 0:  # Only record first episode\n",
    "                frames.append(env.render())\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    print(f\"\\nAverage Reward: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}\")\n",
    "    \n",
    "    return episode_rewards, frames\n",
    "\n",
    "# Quick evaluation (using random policy for demo)\n",
    "print(\"Evaluating agent on CartPole...\")\n",
    "rewards, _ = evaluate_agent(None, \"CartPole-v1\", num_episodes=5, render=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
