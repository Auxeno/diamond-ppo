{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ’Ž Diamond PPO Demo\n",
    "\n",
    "This notebook demonstrates the core features of Diamond PPO, a lightweight PyTorch implementation of Proximal Policy Optimisation.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/auxeno/diamond-ppo/blob/main/notebooks/diamond-ppo-demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, let's install Diamond PPO and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Diamond PPO from GitHub\n",
    "!pip install -q git+https://github.com/auxeno/diamond-ppo\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Gymnasium version: {gym.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Usage - Discrete Actions\n",
    "\n",
    "Let's start with a simple example using CartPole, a classic control task with discrete actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diamond import PPO, PPOConfig\n",
    "\n",
    "# Create a simple PPO agent with custom configuration\n",
    "config = PPOConfig(\n",
    "    total_steps=50_000,  # Total training steps\n",
    "    rollout_steps=128,   # Steps per rollout\n",
    "    num_envs=4,          # Parallel environments\n",
    "    lr=3e-4,             # Learning rate\n",
    "    gamma=0.99,          # Discount factor\n",
    "    gae_lambda=0.95,     # GAE lambda\n",
    "    ppo_clip=0.2,        # PPO clipping parameter\n",
    "    verbose=True         # Print training progress\n",
    ")\n",
    "\n",
    "# Create and train the agent\n",
    "agent = PPO(\n",
    "    env_fn=lambda: gym.make(\"CartPole-v1\"),\n",
    "    cfg=config\n",
    ")\n",
    "\n",
    "print(\"Training PPO on CartPole...\")\n",
    "agent.train()\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Continuous Control\n",
    "\n",
    "For continuous action spaces, use `ContinuousPPO`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diamond import ContinuousPPO, ContinuousPPOConfig\n",
    "\n",
    "# Configure continuous PPO\n",
    "config = ContinuousPPOConfig(\n",
    "    total_steps=100_000,\n",
    "    rollout_steps=256,\n",
    "    num_envs=4,\n",
    "    lr=3e-4,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Train on a continuous control task\n",
    "agent = ContinuousPPO(\n",
    "    env_fn=lambda: gym.make(\"Pendulum-v1\"),\n",
    "    cfg=config\n",
    ")\n",
    "\n",
    "print(\"Training Continuous PPO on Pendulum...\")\n",
    "agent.train()\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Neural Networks\n",
    "\n",
    "Diamond PPO supports custom network architectures. Networks receive the observation space, action space, and config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Example 1: Simple custom network\n",
    "class SimpleCustomNetwork(nn.Module):\n",
    "    def __init__(self, observation_space, action_space, cfg):\n",
    "        super().__init__()\n",
    "        obs_dim = int(np.prod(observation_space.shape))\n",
    "        act_dim = int(action_space.n)\n",
    "        hidden_dim = cfg.network_hidden_dim  # Use config values\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Separate heads\n",
    "        self.actor_head = nn.Linear(hidden_dim, act_dim)\n",
    "        self.critic_head = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def get_actions(self, observations, device):\n",
    "        \"\"\"Returns actions for given observations\"\"\"\n",
    "        x = torch.as_tensor(observations, dtype=torch.float32, device=device)\n",
    "        with torch.inference_mode():\n",
    "            features = self.base(x)\n",
    "            logits = self.actor_head(features)\n",
    "        return torch.distributions.Categorical(logits=logits).sample().cpu().numpy()\n",
    "    \n",
    "    def get_values(self, observations):\n",
    "        \"\"\"Returns value estimates\"\"\"\n",
    "        with torch.inference_mode():\n",
    "            features = self.base(observations)\n",
    "            return self.critic_head(features).squeeze(-1)\n",
    "    \n",
    "    def get_logits_and_values(self, observations):\n",
    "        \"\"\"Returns both logits and values\"\"\"\n",
    "        features = self.base(observations)\n",
    "        logits = self.actor_head(features)\n",
    "        values = self.critic_head(features).squeeze(-1)\n",
    "        return logits, values\n",
    "\n",
    "# Use the simple custom network\n",
    "config = PPOConfig(\n",
    "    total_steps=30_000,\n",
    "    rollout_steps=128,\n",
    "    num_envs=4,\n",
    "    network_hidden_dim=128,  # This will be passed to the network\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent = PPO(\n",
    "    env_fn=lambda: gym.make(\"CartPole-v1\"),\n",
    "    cfg=config,\n",
    "    network_cls=SimpleCustomNetwork\n",
    ")\n",
    "\n",
    "print(\"Training with custom network...\")\n",
    "agent.train()\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Custom config with additional parameters\n",
    "@dataclass\n",
    "class CustomPPOConfig(PPOConfig):\n",
    "    # Inherit all base PPO config fields\n",
    "    # Add custom fields for our network\n",
    "    num_layers: int = 3\n",
    "    use_dropout: bool = True\n",
    "    dropout_rate: float = 0.1\n",
    "    activation: str = \"relu\"\n",
    "\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self, observation_space, action_space, cfg):\n",
    "        super().__init__()\n",
    "        obs_dim = int(np.prod(observation_space.shape))\n",
    "        act_dim = int(action_space.n)\n",
    "        \n",
    "        # Use custom config fields\n",
    "        hidden_dim = cfg.network_hidden_dim\n",
    "        num_layers = cfg.num_layers\n",
    "        dropout_rate = cfg.dropout_rate if cfg.use_dropout else 0.0\n",
    "        \n",
    "        # Build network based on config\n",
    "        activation = nn.ReLU() if cfg.activation == \"relu\" else nn.Tanh()\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_dim = obs_dim if i == 0 else hidden_dim\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(activation)\n",
    "            if cfg.use_dropout:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        self.base = nn.Sequential(*layers)\n",
    "        self.actor_head = nn.Linear(hidden_dim, act_dim)\n",
    "        self.critic_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def get_actions(self, observations, device):\n",
    "        \"\"\"Returns actions for given observations\"\"\"\n",
    "        x = torch.as_tensor(observations, dtype=torch.float32, device=device)\n",
    "        with torch.inference_mode():\n",
    "            features = self.base(x)\n",
    "            logits = self.actor_head(features)\n",
    "        return torch.distributions.Categorical(logits=logits).sample().cpu().numpy()\n",
    "    \n",
    "    def get_values(self, observations):\n",
    "        \"\"\"Returns value estimates\"\"\"\n",
    "        with torch.inference_mode():\n",
    "            features = self.base(observations)\n",
    "            return self.critic_head(features).squeeze(-1)\n",
    "    \n",
    "    def get_logits_and_values(self, observations):\n",
    "        \"\"\"Returns both logits and values\"\"\"\n",
    "        features = self.base(observations)\n",
    "        logits = self.actor_head(features)\n",
    "        values = self.critic_head(features).squeeze(-1)\n",
    "        return logits, values\n",
    "\n",
    "# Create custom config with our parameters\n",
    "custom_config = CustomPPOConfig(\n",
    "    total_steps=30_000,\n",
    "    rollout_steps=128,\n",
    "    num_envs=4,\n",
    "    network_hidden_dim=256,\n",
    "    # Custom parameters\n",
    "    num_layers=4,\n",
    "    use_dropout=True,\n",
    "    dropout_rate=0.2,\n",
    "    activation=\"relu\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create agent with custom config and network\n",
    "agent = PPO(\n",
    "    env_fn=lambda: gym.make(\"CartPole-v1\"),\n",
    "    cfg=custom_config,\n",
    "    network_cls=CustomNetwork\n",
    ")\n",
    "\n",
    "print(f\"Training with {custom_config.num_layers}-layer network, dropout={custom_config.use_dropout}\")\n",
    "agent.train()\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced: Custom Config with Additional Parameters\n",
    "\n",
    "You can extend the config class to add custom parameters for your network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Utilities\n",
    "\n",
    "Diamond PPO includes helpful utilities for monitoring training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diamond.utils import Logger\n",
    "\n",
    "# Example of using the Logger\n",
    "logger = Logger()\n",
    "\n",
    "# Simulate some training metrics\n",
    "np.random.seed(42)\n",
    "for step in range(100):\n",
    "    # Simulated metrics\n",
    "    reward = 100 + step * 2 + np.random.randn() * 10\n",
    "    loss = 1.0 / (1 + step * 0.1) + np.random.randn() * 0.01\n",
    "    \n",
    "    logger.log(\"episode_reward\", step, reward)\n",
    "    logger.log(\"policy_loss\", step, loss)\n",
    "\n",
    "# Plot the logged metrics\n",
    "print(\"Episode Rewards:\")\n",
    "logger.plot(\"episode_reward\")\n",
    "print(\"\\nPolicy Loss:\")\n",
    "logger.plot(\"policy_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the Timer for profiling\n",
    "from diamond.utils import Timer\n",
    "import time\n",
    "\n",
    "timer = Timer()\n",
    "\n",
    "# Simulate different parts of a training loop\n",
    "for i in range(5):\n",
    "    with timer.time(\"environment_step\"):\n",
    "        time.sleep(0.01)  # Simulate env.step()\n",
    "    \n",
    "    with timer.time(\"network_forward\"):\n",
    "        time.sleep(0.005)  # Simulate network forward pass\n",
    "    \n",
    "    with timer.time(\"optimisation\"):\n",
    "        time.sleep(0.008)  # Simulate optimisation step\n",
    "\n",
    "# Display timing statistics\n",
    "timer.plot_timings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation and Visualisation\n",
    "\n",
    "Let's evaluate a trained agent and visualise its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env_name, num_episodes=5, render=False):\n",
    "    \"\"\"Evaluate a trained agent.\"\"\"\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\" if render else None)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    frames = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Get action from trained network\n",
    "            with torch.no_grad():\n",
    "                obs_tensor = torch.as_tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "                if hasattr(agent, \"network\"):\n",
    "                    logits, _ = agent.network(obs_tensor)\n",
    "                    action = torch.argmax(logits, dim=-1).item()\n",
    "                else:\n",
    "                    # For demonstration, use random actions\n",
    "                    action = env.action_space.sample()\n",
    "            \n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += float(reward)\n",
    "            \n",
    "            if render and episode == 0:  # Only record first episode\n",
    "                frames.append(env.render())\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    print(f\"\\nAverage Reward: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}\")\n",
    "    \n",
    "    return episode_rewards, frames\n",
    "\n",
    "# Quick evaluation (using random policy for demo)\n",
    "print(\"Evaluating agent on CartPole...\")\n",
    "rewards, _ = evaluate_agent(None, \"CartPole-v1\", num_episodes=5, render=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
